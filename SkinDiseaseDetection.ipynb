{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 19:04:01.267087: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-20 19:04:01.323208: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-20 19:04:01.323246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-20 19:04:01.328420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-20 19:04:01.354713: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 19:04:02.002534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Input, concatenate\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip HAM10000_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>attribution</th>\n",
       "      <th>copyright_license</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>benign_malignant</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>diagnosis_confirm_type</th>\n",
       "      <th>image_type</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>melanocytic</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM10000_images/ISIC_0024306.JPG</td>\n",
       "      <td>ViDIR Group, Department of Dermatology, Medica...</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>nevus</td>\n",
       "      <td>serial imaging showing no change</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>IL_7252831</td>\n",
       "      <td>True</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM10000_images/ISIC_0024307.JPG</td>\n",
       "      <td>ViDIR Group, Department of Dermatology, Medica...</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>50.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>benign</td>\n",
       "      <td>nevus</td>\n",
       "      <td>serial imaging showing no change</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>IL_6125741</td>\n",
       "      <td>True</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM10000_images/ISIC_0024308.JPG</td>\n",
       "      <td>ViDIR Group, Department of Dermatology, Medica...</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>nevus</td>\n",
       "      <td>serial imaging showing no change</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>IL_3692653</td>\n",
       "      <td>True</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM10000_images/ISIC_0024309.JPG</td>\n",
       "      <td>ViDIR Group, Department of Dermatology, Medica...</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>benign</td>\n",
       "      <td>nevus</td>\n",
       "      <td>serial imaging showing no change</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>IL_0959663</td>\n",
       "      <td>True</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM10000_images/ISIC_0024310.JPG</td>\n",
       "      <td>ViDIR Group, Department of Dermatology, Medica...</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>60.0</td>\n",
       "      <td>anterior torso</td>\n",
       "      <td>malignant</td>\n",
       "      <td>melanoma</td>\n",
       "      <td>histopathology</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>IL_8194852</td>\n",
       "      <td>True</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            isic_id  \\\n",
       "0  HAM10000_images/ISIC_0024306.JPG   \n",
       "1  HAM10000_images/ISIC_0024307.JPG   \n",
       "2  HAM10000_images/ISIC_0024308.JPG   \n",
       "3  HAM10000_images/ISIC_0024309.JPG   \n",
       "4  HAM10000_images/ISIC_0024310.JPG   \n",
       "\n",
       "                                         attribution copyright_license  \\\n",
       "0  ViDIR Group, Department of Dermatology, Medica...          CC-BY-NC   \n",
       "1  ViDIR Group, Department of Dermatology, Medica...          CC-BY-NC   \n",
       "2  ViDIR Group, Department of Dermatology, Medica...          CC-BY-NC   \n",
       "3  ViDIR Group, Department of Dermatology, Medica...          CC-BY-NC   \n",
       "4  ViDIR Group, Department of Dermatology, Medica...          CC-BY-NC   \n",
       "\n",
       "   age_approx anatom_site_general benign_malignant diagnosis  \\\n",
       "0        45.0                 NaN           benign     nevus   \n",
       "1        50.0     lower extremity           benign     nevus   \n",
       "2        55.0                 NaN           benign     nevus   \n",
       "3        40.0                 NaN           benign     nevus   \n",
       "4        60.0      anterior torso        malignant  melanoma   \n",
       "\n",
       "             diagnosis_confirm_type   image_type   lesion_id  melanocytic  \\\n",
       "0  serial imaging showing no change  dermoscopic  IL_7252831         True   \n",
       "1  serial imaging showing no change  dermoscopic  IL_6125741         True   \n",
       "2  serial imaging showing no change  dermoscopic  IL_3692653         True   \n",
       "3  serial imaging showing no change  dermoscopic  IL_0959663         True   \n",
       "4                    histopathology  dermoscopic  IL_8194852         True   \n",
       "\n",
       "      sex  \n",
       "0    male  \n",
       "1    male  \n",
       "2  female  \n",
       "3    male  \n",
       "4    male  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('metadata.csv')\n",
    "metadata['isic_id'] = metadata['isic_id'].apply(lambda x: f\"{x}.JPG\")\n",
    "image_path = 'HAM10000_images/'\n",
    "metadata['isic_id'] = image_path + metadata['isic_id']\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             nevus\n",
       "1                             nevus\n",
       "2                             nevus\n",
       "3                             nevus\n",
       "4                          melanoma\n",
       "                    ...            \n",
       "11715    pigmented benign keratosis\n",
       "11716                         nevus\n",
       "11717             actinic keratosis\n",
       "11718    pigmented benign keratosis\n",
       "11719          basal cell carcinoma\n",
       "Name: diagnosis, Length: 11720, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['isic_id', 'attribution', 'copyright_license', 'age_approx',\n",
       "       'anatom_site_general', 'benign_malignant', 'diagnosis',\n",
       "       'diagnosis_confirm_type', 'image_type', 'lesion_id', 'melanocytic',\n",
       "       'sex'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata= metadata.drop(['attribution', 'copyright_license', 'diagnosis_confirm_type', 'anatom_site_general','image_type', 'lesion_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11720 entries, 0 to 11719\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   isic_id           11720 non-null  object \n",
      " 1   age_approx        11337 non-null  float64\n",
      " 2   benign_malignant  9042 non-null   object \n",
      " 3   diagnosis         11720 non-null  object \n",
      " 4   melanocytic       11720 non-null  bool   \n",
      " 5   sex               11377 non-null  object \n",
      "dtypes: bool(1), float64(1), object(4)\n",
      "memory usage: 469.4+ KB\n"
     ]
    }
   ],
   "source": [
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nevus', 'melanoma', 'pigmented benign keratosis',\n",
       "       'dermatofibroma', 'squamous cell carcinoma',\n",
       "       'basal cell carcinoma', 'vascular lesion', 'actinic keratosis'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val= train_test_split(metadata, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['basal cell carcinoma', 'pigmented benign keratosis', 'nevus',\n",
       "       'melanoma', 'squamous cell carcinoma', 'dermatofibroma',\n",
       "       'actinic keratosis', 'vascular lesion'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nevus', 'melanoma', 'actinic keratosis', 'basal cell carcinoma',\n",
       "       'pigmented benign keratosis', 'squamous cell carcinoma',\n",
       "       'vascular lesion', 'dermatofibroma'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[\"diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9376 validated image filenames belonging to 8 classes.\n",
      "Found 2344 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "s = (100,75)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    directory=None,\n",
    "    x_col=\"isic_id\",\n",
    "    y_col=\"diagnosis\",  # change here\n",
    "    subset=\"training\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",  # change here\n",
    "    target_size=s,\n",
    ")\n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=val,\n",
    "    directory=None,\n",
    "    x_col=\"isic_id\",\n",
    "    y_col=\"diagnosis\",  # change here\n",
    "    subset=\"training\",\n",
    "    batch_size=32,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode=\"categorical\",  # change here\n",
    "    target_size=s,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain=train.drop([\"isic_id\", \"diagnosis\"], axis=1)\n",
    "Xval=val.drop([\"isic_id\", \"diagnosis\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['basal cell carcinoma', 'pigmented benign keratosis', 'nevus',\n",
       "       'melanoma', 'squamous cell carcinoma', 'dermatofibroma',\n",
       "       'actinic keratosis', 'vascular lesion'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"diagnosis\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 206ms/step\n",
      "The predicted label is: nevus\n"
     ]
    }
   ],
   "source": [
    "from django.shortcuts import render\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "get_custom_objects().update({'f1_score': f1_score})\n",
    "image = \"HAM10000_images/ISIC_0024306.JPG\"\n",
    "\n",
    "# Open the image and resize it\n",
    "img = Image.open(image).convert('RGB')\n",
    "img = img.resize((100, 75))\n",
    "img=img.rotate(180)\n",
    "# # # Convert the image to numpy array and normalize it\n",
    "img_array = np.array(img) / 255\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "img_array = np.swapaxes(img_array, 1, 2)\n",
    "\n",
    "# # # # Load the model\n",
    "model = load_model(\"best_model.h5\")\n",
    "\n",
    "prediction = model.predict(img_array)\n",
    "label_index = np.argmax(prediction)\n",
    "labels = dict((v,k) for k,v in train_generator.class_indices.items())\n",
    "predicted_label = labels[label_index]\n",
    "\n",
    "print(f\"The predicted label is: {predicted_label}\")\n",
    "# # img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'actinic keratosis',\n",
       " 1: 'basal cell carcinoma',\n",
       " 2: 'dermatofibroma',\n",
       " 3: 'melanoma',\n",
       " 4: 'nevus',\n",
       " 5: 'pigmented benign keratosis',\n",
       " 6: 'squamous cell carcinoma',\n",
       " 7: 'vascular lesion'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict((v,k) for k,v in train_generator.class_indices.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 18:21:17.641556: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-04-20 18:21:17.825910: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n",
      "2024-04-20 18:21:18.010217: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: Permission denied\n",
      "2024-04-20 18:21:19.212141: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f057c223f80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-20 18:21:19.212183: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Ti Laptop GPU, Compute Capability 8.6\n",
      "2024-04-20 18:21:19.229957: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713617479.340597   19769 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293/293 [==============================] - ETA: 0s - loss: 0.9814 - accuracy: 0.6796 - f1_score: 0.6728\n",
      "Epoch 1: val_f1_score improved from -inf to 0.69558, saving model to best_model.h5\n",
      "293/293 [==============================] - 18s 50ms/step - loss: 0.9814 - accuracy: 0.6796 - f1_score: 0.6728 - val_loss: 0.8112 - val_accuracy: 0.7014 - val_f1_score: 0.6956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.8050 - accuracy: 0.7200 - f1_score: 0.7131\n",
      "Epoch 2: val_f1_score improved from 0.69558 to 0.72837, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.8050 - accuracy: 0.7200 - f1_score: 0.7131 - val_loss: 0.7868 - val_accuracy: 0.7223 - val_f1_score: 0.7284\n",
      "Epoch 3/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.7120 - accuracy: 0.7509 - f1_score: 0.7454\n",
      "Epoch 3: val_f1_score improved from 0.72837 to 0.74319, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.7120 - accuracy: 0.7509 - f1_score: 0.7454 - val_loss: 0.7210 - val_accuracy: 0.7466 - val_f1_score: 0.7432\n",
      "Epoch 4/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.7735 - f1_score: 0.7676\n",
      "Epoch 4: val_f1_score did not improve from 0.74319\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.6447 - accuracy: 0.7735 - f1_score: 0.7676 - val_loss: 0.7732 - val_accuracy: 0.7402 - val_f1_score: 0.7356\n",
      "Epoch 5/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.7938 - f1_score: 0.7942\n",
      "Epoch 5: val_f1_score did not improve from 0.74319\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.5836 - accuracy: 0.7938 - f1_score: 0.7942 - val_loss: 0.7939 - val_accuracy: 0.7308 - val_f1_score: 0.7366\n",
      "Epoch 6/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.8116 - f1_score: 0.8140\n",
      "Epoch 6: val_f1_score improved from 0.74319 to 0.74652, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 49ms/step - loss: 0.5247 - accuracy: 0.8116 - f1_score: 0.8140 - val_loss: 0.8267 - val_accuracy: 0.7500 - val_f1_score: 0.7465\n",
      "Epoch 7/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.8299 - f1_score: 0.8279\n",
      "Epoch 7: val_f1_score did not improve from 0.74652\n",
      "293/293 [==============================] - 15s 50ms/step - loss: 0.4834 - accuracy: 0.8299 - f1_score: 0.8279 - val_loss: 0.8170 - val_accuracy: 0.7393 - val_f1_score: 0.7283\n",
      "Epoch 8/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8625 - f1_score: 0.8611\n",
      "Epoch 8: val_f1_score did not improve from 0.74652\n",
      "293/293 [==============================] - 15s 50ms/step - loss: 0.3956 - accuracy: 0.8625 - f1_score: 0.8611 - val_loss: 0.8594 - val_accuracy: 0.7440 - val_f1_score: 0.7445\n",
      "Epoch 9/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8757 - f1_score: 0.8751\n",
      "Epoch 9: val_f1_score improved from 0.74652 to 0.76114, saving model to best_model.h5\n",
      "293/293 [==============================] - 15s 51ms/step - loss: 0.3544 - accuracy: 0.8757 - f1_score: 0.8751 - val_loss: 0.9068 - val_accuracy: 0.7581 - val_f1_score: 0.7611\n",
      "Epoch 10/50\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.8982 - f1_score: 0.8997\n",
      "Epoch 10: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 15s 50ms/step - loss: 0.2807 - accuracy: 0.8985 - f1_score: 0.9000 - val_loss: 0.9881 - val_accuracy: 0.7611 - val_f1_score: 0.7610\n",
      "Epoch 11/50\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.2480 - accuracy: 0.9140 - f1_score: 0.9143\n",
      "Epoch 11: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 15s 51ms/step - loss: 0.2489 - accuracy: 0.9134 - f1_score: 0.9138 - val_loss: 0.9808 - val_accuracy: 0.7137 - val_f1_score: 0.7160\n",
      "Epoch 12/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.2153 - accuracy: 0.9274 - f1_score: 0.9283\n",
      "Epoch 12: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 15s 52ms/step - loss: 0.2153 - accuracy: 0.9274 - f1_score: 0.9283 - val_loss: 1.2490 - val_accuracy: 0.7564 - val_f1_score: 0.7583\n",
      "Epoch 13/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9387 - f1_score: 0.9402\n",
      "Epoch 13: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 16s 53ms/step - loss: 0.1830 - accuracy: 0.9387 - f1_score: 0.9402 - val_loss: 1.0083 - val_accuracy: 0.7504 - val_f1_score: 0.7556\n",
      "Epoch 14/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9396 - f1_score: 0.9400\n",
      "Epoch 14: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 15s 53ms/step - loss: 0.1674 - accuracy: 0.9396 - f1_score: 0.9400 - val_loss: 1.2678 - val_accuracy: 0.7538 - val_f1_score: 0.7540\n",
      "Epoch 15/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1283 - accuracy: 0.9580 - f1_score: 0.9581\n",
      "Epoch 15: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 15s 53ms/step - loss: 0.1283 - accuracy: 0.9580 - f1_score: 0.9581 - val_loss: 1.2712 - val_accuracy: 0.7496 - val_f1_score: 0.7536\n",
      "Epoch 16/50\n",
      "292/293 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9604 - f1_score: 0.9607\n",
      "Epoch 16: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 16s 55ms/step - loss: 0.1149 - accuracy: 0.9604 - f1_score: 0.9607 - val_loss: 1.3256 - val_accuracy: 0.7491 - val_f1_score: 0.7507\n",
      "Epoch 17/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9608 - f1_score: 0.9613\n",
      "Epoch 17: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 16s 53ms/step - loss: 0.1221 - accuracy: 0.9608 - f1_score: 0.9613 - val_loss: 1.6679 - val_accuracy: 0.7440 - val_f1_score: 0.7451\n",
      "Epoch 18/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9572 - f1_score: 0.9577\n",
      "Epoch 18: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 14s 49ms/step - loss: 0.1280 - accuracy: 0.9572 - f1_score: 0.9577 - val_loss: 1.3107 - val_accuracy: 0.7372 - val_f1_score: 0.7390\n",
      "Epoch 19/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9677 - f1_score: 0.9681\n",
      "Epoch 19: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 14s 49ms/step - loss: 0.0964 - accuracy: 0.9677 - f1_score: 0.9681 - val_loss: 1.5179 - val_accuracy: 0.7453 - val_f1_score: 0.7482\n",
      "Epoch 20/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9717 - f1_score: 0.9713\n",
      "Epoch 20: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 14s 48ms/step - loss: 0.0810 - accuracy: 0.9717 - f1_score: 0.9713 - val_loss: 1.4362 - val_accuracy: 0.7406 - val_f1_score: 0.7414\n",
      "Epoch 21/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9786 - f1_score: 0.9789\n",
      "Epoch 21: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0624 - accuracy: 0.9786 - f1_score: 0.9789 - val_loss: 1.5702 - val_accuracy: 0.7274 - val_f1_score: 0.7300\n",
      "Epoch 22/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9758 - f1_score: 0.9760\n",
      "Epoch 22: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0756 - accuracy: 0.9758 - f1_score: 0.9760 - val_loss: 1.6109 - val_accuracy: 0.7359 - val_f1_score: 0.7362\n",
      "Epoch 23/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9708 - f1_score: 0.9711\n",
      "Epoch 23: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0835 - accuracy: 0.9708 - f1_score: 0.9711 - val_loss: 1.5122 - val_accuracy: 0.7530 - val_f1_score: 0.7545\n",
      "Epoch 24/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9808 - f1_score: 0.9808\n",
      "Epoch 24: val_f1_score did not improve from 0.76114\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0582 - accuracy: 0.9808 - f1_score: 0.9808 - val_loss: 1.5809 - val_accuracy: 0.7479 - val_f1_score: 0.7518\n",
      "Epoch 25/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9739 - f1_score: 0.9738\n",
      "Epoch 25: val_f1_score improved from 0.76114 to 0.76184, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.0755 - accuracy: 0.9739 - f1_score: 0.9738 - val_loss: 1.4487 - val_accuracy: 0.7598 - val_f1_score: 0.7618\n",
      "Epoch 26/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9861 - f1_score: 0.9861\n",
      "Epoch 26: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0422 - accuracy: 0.9861 - f1_score: 0.9861 - val_loss: 1.7203 - val_accuracy: 0.7470 - val_f1_score: 0.7481\n",
      "Epoch 27/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9738 - f1_score: 0.9737\n",
      "Epoch 27: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0781 - accuracy: 0.9738 - f1_score: 0.9737 - val_loss: 1.5821 - val_accuracy: 0.7615 - val_f1_score: 0.7614\n",
      "Epoch 28/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9844 - f1_score: 0.9845\n",
      "Epoch 28: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0450 - accuracy: 0.9844 - f1_score: 0.9845 - val_loss: 1.6658 - val_accuracy: 0.7415 - val_f1_score: 0.7395\n",
      "Epoch 29/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9753 - f1_score: 0.9748\n",
      "Epoch 29: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0710 - accuracy: 0.9753 - f1_score: 0.9748 - val_loss: 1.5821 - val_accuracy: 0.7504 - val_f1_score: 0.7487\n",
      "Epoch 30/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9860 - f1_score: 0.9859\n",
      "Epoch 30: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0416 - accuracy: 0.9860 - f1_score: 0.9859 - val_loss: 1.9138 - val_accuracy: 0.7581 - val_f1_score: 0.7569\n",
      "Epoch 31/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9855 - f1_score: 0.9854\n",
      "Epoch 31: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0448 - accuracy: 0.9855 - f1_score: 0.9854 - val_loss: 1.7512 - val_accuracy: 0.7530 - val_f1_score: 0.7537\n",
      "Epoch 32/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9765 - f1_score: 0.9766\n",
      "Epoch 32: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.1056 - accuracy: 0.9765 - f1_score: 0.9766 - val_loss: 1.9783 - val_accuracy: 0.7500 - val_f1_score: 0.7498\n",
      "Epoch 33/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9879 - f1_score: 0.9879\n",
      "Epoch 33: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0350 - accuracy: 0.9879 - f1_score: 0.9879 - val_loss: 1.8759 - val_accuracy: 0.7440 - val_f1_score: 0.7461\n",
      "Epoch 34/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9883 - f1_score: 0.9879\n",
      "Epoch 34: val_f1_score did not improve from 0.76184\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0372 - accuracy: 0.9883 - f1_score: 0.9879 - val_loss: 2.0806 - val_accuracy: 0.7560 - val_f1_score: 0.7552\n",
      "Epoch 35/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9859 - f1_score: 0.9857\n",
      "Epoch 35: val_f1_score improved from 0.76184 to 0.76277, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.0455 - accuracy: 0.9859 - f1_score: 0.9857 - val_loss: 1.6709 - val_accuracy: 0.7598 - val_f1_score: 0.7628\n",
      "Epoch 36/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9829 - f1_score: 0.9833\n",
      "Epoch 36: val_f1_score did not improve from 0.76277\n",
      "293/293 [==============================] - 14s 48ms/step - loss: 0.0485 - accuracy: 0.9829 - f1_score: 0.9833 - val_loss: 1.8924 - val_accuracy: 0.7491 - val_f1_score: 0.7506\n",
      "Epoch 37/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9873 - f1_score: 0.9872\n",
      "Epoch 37: val_f1_score did not improve from 0.76277\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.0407 - accuracy: 0.9873 - f1_score: 0.9872 - val_loss: 1.9800 - val_accuracy: 0.7551 - val_f1_score: 0.7549\n",
      "Epoch 38/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9878 - f1_score: 0.9878\n",
      "Epoch 38: val_f1_score improved from 0.76277 to 0.76342, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 48ms/step - loss: 0.0368 - accuracy: 0.9878 - f1_score: 0.9878 - val_loss: 1.9506 - val_accuracy: 0.7632 - val_f1_score: 0.7634\n",
      "Epoch 39/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9757 - f1_score: 0.9758\n",
      "Epoch 39: val_f1_score did not improve from 0.76342\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.0914 - accuracy: 0.9757 - f1_score: 0.9758 - val_loss: 1.8611 - val_accuracy: 0.7530 - val_f1_score: 0.7539\n",
      "Epoch 40/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9825 - f1_score: 0.9825\n",
      "Epoch 40: val_f1_score did not improve from 0.76342\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0581 - accuracy: 0.9825 - f1_score: 0.9825 - val_loss: 1.7420 - val_accuracy: 0.7543 - val_f1_score: 0.7578\n",
      "Epoch 41/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9908 - f1_score: 0.9909\n",
      "Epoch 41: val_f1_score did not improve from 0.76342\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0335 - accuracy: 0.9908 - f1_score: 0.9909 - val_loss: 2.0355 - val_accuracy: 0.7585 - val_f1_score: 0.7619\n",
      "Epoch 42/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9902 - f1_score: 0.9900\n",
      "Epoch 42: val_f1_score did not improve from 0.76342\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0344 - accuracy: 0.9902 - f1_score: 0.9900 - val_loss: 1.8457 - val_accuracy: 0.7504 - val_f1_score: 0.7508\n",
      "Epoch 43/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9847 - f1_score: 0.9852\n",
      "Epoch 43: val_f1_score did not improve from 0.76342\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0482 - accuracy: 0.9847 - f1_score: 0.9852 - val_loss: 2.1686 - val_accuracy: 0.7453 - val_f1_score: 0.7450\n",
      "Epoch 44/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9861 - f1_score: 0.9864\n",
      "Epoch 44: val_f1_score improved from 0.76342 to 0.76458, saving model to best_model.h5\n",
      "293/293 [==============================] - 14s 47ms/step - loss: 0.0451 - accuracy: 0.9861 - f1_score: 0.9864 - val_loss: 1.9636 - val_accuracy: 0.7628 - val_f1_score: 0.7646\n",
      "Epoch 45/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9922 - f1_score: 0.9923\n",
      "Epoch 45: val_f1_score did not improve from 0.76458\n",
      "293/293 [==============================] - 14s 46ms/step - loss: 0.0280 - accuracy: 0.9922 - f1_score: 0.9923 - val_loss: 2.1253 - val_accuracy: 0.7538 - val_f1_score: 0.7578\n",
      "Epoch 46/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9894 - f1_score: 0.9895\n",
      "Epoch 46: val_f1_score did not improve from 0.76458\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0319 - accuracy: 0.9894 - f1_score: 0.9895 - val_loss: 1.7821 - val_accuracy: 0.7445 - val_f1_score: 0.7447\n",
      "Epoch 47/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9897 - f1_score: 0.9895\n",
      "Epoch 47: val_f1_score did not improve from 0.76458\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0326 - accuracy: 0.9897 - f1_score: 0.9895 - val_loss: 2.2736 - val_accuracy: 0.7581 - val_f1_score: 0.7610\n",
      "Epoch 48/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9889 - f1_score: 0.9891\n",
      "Epoch 48: val_f1_score did not improve from 0.76458\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0385 - accuracy: 0.9889 - f1_score: 0.9891 - val_loss: 2.1359 - val_accuracy: 0.7534 - val_f1_score: 0.7550\n",
      "Epoch 49/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9878 - f1_score: 0.9880\n",
      "Epoch 49: val_f1_score did not improve from 0.76458\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0411 - accuracy: 0.9878 - f1_score: 0.9880 - val_loss: 1.7503 - val_accuracy: 0.7432 - val_f1_score: 0.7461\n",
      "Epoch 50/50\n",
      "293/293 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9852 - f1_score: 0.9851\n",
      "Epoch 50: val_f1_score did not improve from 0.76458\n",
      "293/293 [==============================] - 13s 46ms/step - loss: 0.0524 - accuracy: 0.9852 - f1_score: 0.9851 - val_loss: 1.9736 - val_accuracy: 0.7351 - val_f1_score: 0.7388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f06223d2a10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "# Define a function to calculate f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "\n",
    "get_custom_objects().update({'f1_score': f1_score})\n",
    "\n",
    "\n",
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(100, 75, 3))\n",
    "\n",
    "# Freeze all layers in the base model except the last four\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top of the base model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "num_classes = len(train_generator.class_indices)  # change here\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score])\n",
    "\n",
    "# Create a callback that saves the best model observed on the validation data\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, validation_data=valid_generator, epochs=50, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
